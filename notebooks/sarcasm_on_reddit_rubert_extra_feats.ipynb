{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RuBERT with extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ms/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ms/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to /home/ms/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/ms/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import OrderedDict\n",
    "from scikitplot.metrics import plot_precision_recall_curve, plot_roc_curve\n",
    "from sklearn.metrics import (f1_score, precision_score, average_precision_score, roc_auc_score,\n",
    "                             classification_report, accuracy_score, make_scorer,\n",
    "                             precision_recall_curve, recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sarcsdet.utils.train_utils import *\n",
    "\n",
    "from deeppavlov.core.common.file import read_json\n",
    "from deeppavlov import build_model, configs, train_model\n",
    "from deeppavlov.models.torch_bert.torch_transformers_classifier import TorchTransformersClassifierModel\n",
    "from deeppavlov.models.preprocessors.torch_transformers_preprocessor import TorchTransformersPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/Sarcasm_on_Reddit/rus-train-balanced-sarcasm-ling_feat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=8)\n",
    "train_df, valid_df = train_test_split(train_df, test_size=0.1, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test_classification_metrics(y_test, y_pred, y_pred_prob, X_test=None, classifier=None):\n",
    "    print(f\"F1: {f1_score(y_test, y_pred):.5}\")\n",
    "    print(f\"PREC: {precision_score(y_test, y_pred):.5}\")\n",
    "    print(f\"PR-AUC: {average_precision_score(y_test, y_pred_prob):.5}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_prob):.5}\")\n",
    "    print('-------------------------------------------------------')\n",
    "    print(classification_report(y_test, y_pred, labels=[0, 1]))\n",
    "    print('-------------------------------------------------------')\n",
    "    if classifier:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        ax[0].set_title('Precision-Recall curve')\n",
    "        plot_precision_recall_curve(classifier, X_test, y_test, ax=ax[0])\n",
    "        ax[1].set_title('ROC-AUC curve')\n",
    "        plot_roc_curve(classifier, X_test, y_test, ax=ax[1])\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_preds(df, bert_model):\n",
    "    preds_proba = []\n",
    "    for batch in tqdm(chunks(df['rus_comment'].values, 64), total=int(df.index.size / 64)):\n",
    "        preds_proba.append(bert_model(batch))\n",
    "\n",
    "    preds = np.concatenate(preds_proba)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "  'score', 'author', 'subreddit',\n",
    "  'exclamation', 'question', 'quotes', 'dotes', \n",
    "  'funny_mark', 'interjections'\n",
    "  ]\n",
    "\n",
    "X = train_df[features].values\n",
    "y = train_df['label'].values\n",
    "\n",
    "test_X = test_df[features].values\n",
    "test_y = test_df['label'].values\n",
    "\n",
    "valid_X = valid_df[features].values\n",
    "valid_y = valid_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu', input_shape=(X.shape[1], )))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 522314 samples, validate on 58035 samples\n",
      "Epoch 1/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 112.9807 - auc: 0.5064 - val_loss: 17.7023 - val_auc: 0.5074\n",
      "Epoch 2/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 24.1305 - auc: 0.5099 - val_loss: 1.4438 - val_auc: 0.5530\n",
      "Epoch 3/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 23.2210 - auc: 0.5104 - val_loss: 33.9796 - val_auc: 0.5032\n",
      "Epoch 4/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 23.8328 - auc: 0.5124 - val_loss: 18.0086 - val_auc: 0.5080\n",
      "Epoch 5/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 21.9739 - auc: 0.5120 - val_loss: 25.3481 - val_auc: 0.5070\n",
      "Epoch 6/10\n",
      "522314/522314 [==============================] - 10s 20us/sample - loss: 21.6755 - auc: 0.5137 - val_loss: 23.9952 - val_auc: 0.5049\n",
      "Epoch 7/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 21.3915 - auc: 0.5138 - val_loss: 1.8149 - val_auc: 0.5277\n",
      "Epoch 8/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 21.3175 - auc: 0.5133 - val_loss: 10.5983 - val_auc: 0.5209\n",
      "Epoch 9/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 20.5314 - auc: 0.5153 - val_loss: 26.5756 - val_auc: 0.5108\n",
      "Epoch 10/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 19.3003 - auc: 0.5161 - val_loss: 19.4133 - val_auc: 0.5120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0cd3378e90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, validation_data=(valid_X, valid_y), epochs=10, workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RuBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/bert_dp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:193: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/deeppavlov/models/bert/bert_classifier.py:84: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/deeppavlov/models/bert/bert_classifier.py:161: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/bert_dp/modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/bert_dp/modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/bert_dp/modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/bert_dp/modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/bert_dp/modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/bert_dp/modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:127: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:127: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/deeppavlov/models/bert/bert_classifier.py:92: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/deeppavlov/models/bert/bert_classifier.py:97: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "2021-04-28 02:51:01.240 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /home/ms/Desktop/kate/Sarcasm_Detection/data/Models/reddit/rubert/model]\n",
      "WARNING:tensorflow:From /home/ms/miniconda3/lib/python3.7/site-packages/deeppavlov/core/models/tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /home/ms/Desktop/kate/Sarcasm_Detection/data/Models/reddit/rubert/model\n"
     ]
    }
   ],
   "source": [
    "bert_config = read_json(configs.classifiers.rusentiment_bert)\n",
    "\n",
    "bert_config['dataset_reader']['x'] = 'rus_comment'\n",
    "bert_config['dataset_reader']['y'] = 'label'\n",
    "bert_config['dataset_reader']['data_path'] = './'\n",
    "bert_config['dataset_reader']['train'] = 'train.csv'\n",
    "bert_config['dataset_reader']['valid'] = 'valid.csv'\n",
    "bert_config['dataset_reader']['test'] = 'test.csv'\n",
    "\n",
    "del bert_config['dataset_iterator']['split_seed']\n",
    "del bert_config['dataset_iterator']['field_to_split']\n",
    "del bert_config['dataset_iterator']['split_fields']\n",
    "del bert_config['dataset_iterator']['split_proportions']\n",
    "\n",
    "bert_config['metadata']['variables']['MODEL_PATH'] = '../data/Models/reddit/rubert/'\n",
    "\n",
    "del bert_config['chainer']['pipe'][-2:]\n",
    "del bert_config['chainer']['pipe'][1]\n",
    "bert_config['chainer']['pipe'][1]['in'] = 'y'\n",
    "bert_config['chainer']['pipe'][1]['depth'] = 2\n",
    "bert_config['chainer']['pipe'][2]['n_classes'] = 2\n",
    "bert_config['train']['metrics'] = [bert_config['train']['metrics'][-1]]\n",
    "bert_config['chainer']['out'] = ['y_pred_probas']\n",
    "bert_config['train']['epochs'] = 2\n",
    "bert_config['train']['batch_size'] = 32\n",
    "bert_config['train']['show_examples'] = True\n",
    "\n",
    "vocab_file = '{DOWNLOADS_PATH}/bert_models/rubert_cased_L-12_H-768_A-12_v1/vocab.txt'\n",
    "bert_config_file = \"{DOWNLOADS_PATH}/bert_models/rubert_cased_L-12_H-768_A-12_v1/bert_config.json\"\n",
    "pretrained_bert = \"{DOWNLOADS_PATH}/bert_models/rubert_cased_L-12_H-768_A-12_v1/bert_model.ckpt\"\n",
    "\n",
    "bert_config['chainer']['pipe'][0]['vocab_file'] = vocab_file\n",
    "bert_config['chainer']['pipe'][1]['bert_config_file'] = bert_config_file\n",
    "bert_config['chainer']['pipe'][1]['pretrained_bert'] = pretrained_bert\n",
    "\n",
    "bert_config['chainer']['pipe'][2]['bert_config_file'] = bert_config_file\n",
    "bert_config['chainer']['pipe'][2]['pretrained_bert'] = pretrained_bert\n",
    "\n",
    "bert_model = build_model(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8162it [5:47:05,  2.55s/it]\n",
      "907it [38:23,  2.54s/it]\n",
      "3887it [2:47:48,  2.59s/it]\n"
     ]
    }
   ],
   "source": [
    "bert_X = get_bert_preds(train_df, bert_model)\n",
    "bert_valid_X = get_bert_preds(valid_df, bert_model)\n",
    "bert_test_X = get_bert_preds(test_df, bert_model)\n",
    "\n",
    "bert_X = bert_X[:, 0]\n",
    "bert_valid_X = bert_valid_X[:, 0]\n",
    "bert_test_X = bert_test_X[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = np.concatenate([bert_X.reshape((-1, 1)), model.predict(X)], axis=1)\n",
    "valid_XX = np.concatenate([bert_valid_X.reshape((-1, 1)), model.predict(valid_X)], axis=1)\n",
    "test_XX = np.concatenate([bert_test_X.reshape((-1, 1)), model.predict(test_X)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Dense(10, activation='relu', input_shape=(2, )))\n",
    "model2.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 522314 samples, validate on 58035 samples\n",
      "Epoch 1/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 0.4641 - auc_1: 0.8631 - val_loss: 0.4537 - val_auc_1: 0.8706\n",
      "Epoch 2/10\n",
      "522314/522314 [==============================] - 10s 20us/sample - loss: 0.4569 - auc_1: 0.8686 - val_loss: 0.4538 - val_auc_1: 0.8706\n",
      "Epoch 3/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 0.4569 - auc_1: 0.8686 - val_loss: 0.4539 - val_auc_1: 0.8706\n",
      "Epoch 4/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 0.4569 - auc_1: 0.8686 - val_loss: 0.4540 - val_auc_1: 0.8705\n",
      "Epoch 5/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 0.4569 - auc_1: 0.8686 - val_loss: 0.4539 - val_auc_1: 0.8705\n",
      "Epoch 6/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 0.4569 - auc_1: 0.8686 - val_loss: 0.4545 - val_auc_1: 0.8705\n",
      "Epoch 7/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 0.4569 - auc_1: 0.8686 - val_loss: 0.4537 - val_auc_1: 0.8705\n",
      "Epoch 8/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 0.4569 - auc_1: 0.8686 - val_loss: 0.4538 - val_auc_1: 0.8705\n",
      "Epoch 9/10\n",
      "522314/522314 [==============================] - 10s 19us/sample - loss: 0.4569 - auc_1: 0.8686 - val_loss: 0.4544 - val_auc_1: 0.8706\n",
      "Epoch 10/10\n",
      "522314/522314 [==============================] - 10s 20us/sample - loss: 0.4569 - auc_1: 0.8686 - val_loss: 0.4538 - val_auc_1: 0.8705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0bd74299d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(XX, y, validation_data=(valid_XX, valid_y), epochs=10, workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.78267\n",
      "PREC: 0.82391\n",
      "PR-AUC: 0.88092\n",
      "ROC-AUC: 0.86852\n",
      "-------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.83      0.79    119653\n",
      "           1       0.82      0.75      0.78    129068\n",
      "\n",
      "    accuracy                           0.79    248721\n",
      "   macro avg       0.79      0.79      0.79    248721\n",
      "weighted avg       0.79      0.79      0.79    248721\n",
      "\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "preds = model2.predict(test_XX)\n",
    "\n",
    "show_test_classification_metrics(\n",
    "    test_y, \n",
    "    (preds > 0.55).astype(int), \n",
    "    preds\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "afd1948f0f7848887a59cfcfa6cfec8a40b28dfb2d2f8aa7f39d605b4ba9656d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
